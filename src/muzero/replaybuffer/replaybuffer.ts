import {GameHistory} from '../selfplay/gamehistory'
import {type Playerwise} from '../selfplay/entities'
import {Batch} from './batch'
import {MuZeroGameSample} from './gamesample'
import {MuZeroPositionSample} from './positionsample'
import fs from 'fs'
import debugFactory from 'debug'
import {type Environment} from '../games/core/environment'
import {type Config} from '../games/core/config'
import * as tf from '@tensorflow/tfjs-node'

const debug = debugFactory('muzero:replaybuffer:module')

/**
 * Replay Buffer
 */

export class ReplayBuffer<State extends Playerwise> {
    public numPlayedGames: number
    public numPlayedSteps: number
    public totalSamples: number
    private buffer: Array<GameHistory<State>>

    /**
     *
     * @param config
     * @param config.replayBufferSize Number of self-play games to keep in the replay buffer
     * @param config.actionSpace Number of all possible actions
     * @param config.tdSteps Number of steps in the future to take into account for calculating
     * the target value
     * @param config.batchSize Number of parts of games to train on at each training step
     * @param config.numUnrollSteps Number of game moves to keep for every batch element
     * @param config.stackedObservations Number of previous observations and previous actions
     * to add to the current observation
     * @param config.discount Chronological discount of the reward
     * @param config.prioritizedReplay Prioritized Replay (See paper appendix Training),
     * select in priority the elements in the replay buffer which are unexpected for the network
     * @param config.priorityAlpha How much prioritization is used, 0 corresponding to the uniform case,
     * paper suggests 1.0
     */
    public constructor(
        private readonly config: Config
    ) {
        this.buffer = []
        this.numPlayedGames = 0
        this.numPlayedSteps = 0
        this.totalSamples = 0
    }

    get totalGames(): number {
        return this.buffer.length
    }

    /**
     * saveGame
     * Save a game in the replay buffer (generated by selfPlay)
     * @param gameHistory
     */
    public saveGame(gameHistory: GameHistory<State>): void {
        if (this.config.prioritizedReplay) {
            // Initial priorities for the prioritized replay (See paper appendix Training)
            // For each game position calculate the absolute deviation from target value. Largest deviation has priority
            const priorities: number[] = []
            gameHistory.rootValues.forEach((rootValue, i) => {
                const targetValue = gameHistory.computeTargetValue(i, this.config.tdSteps, this.config.discount)
                const priority = Math.pow(Math.abs(rootValue - targetValue), this.config.priorityAlpha)
                priorities.push(priority)
            })
            // Deviations are saved for later priority sorting
            gameHistory.priorities = priorities
            gameHistory.gamePriority = gameHistory.priorities.reduce((m, p) => Math.max(m, p), 0)
        }
        if (this.buffer.length >= this.config.replayBufferSize) {
            const delGameHistory = this.buffer.shift()
            if (delGameHistory != null) {
                this.totalSamples -= delGameHistory.rootValues.length
            }
        }
        this.buffer.push(gameHistory)
        this.numPlayedGames++
        this.numPlayedSteps += gameHistory.rootValues.length
        this.totalSamples += gameHistory.rootValues.length
        if (this.numPlayedGames % 25 === 0) {
            //      this.storeSavedGames()
        }
    }

    /**
     * sampleBatch
     * Get a sample batch from the replay buffer (used for training)
     * @param numUnrollSteps Number of game moves to keep for every batch element
     * @param tdSteps Number of steps in the future to take into account for calculating the target value
     * @param forceUniform Flag to force a uniform selection randomly
     * @return MuZeroBatch[] Sample batch - list of batch elements (batchSize length)
     */
    public sampleBatch(numUnrollSteps: number, tdSteps: number, forceUniform = false): Batch[] {
        const gameSamples: Array<MuZeroGameSample<State>> = []
        if (this.numPlayedGames > 0) {
            for (let c = 0; c < (this.config.batchSize); c++) {
                gameSamples.push(this.sampleGame(forceUniform))
            }
        }
        //    debug('Sample %d games', this.batchSize)
        return gameSamples.map(g => {
            const gameHistory = g.gameHistory
            //      debug(game.env.toString(game.state))
            //      debug(game.state.toString())
            const position = this.samplePosition(gameHistory, forceUniform).position
            const actionHistory = gameHistory.actionHistory.slice(position, position + numUnrollSteps)
            const tfActionHistory = actionHistory.map(
                action => tf.squeeze(
                    tf.oneHot(
                        tf.tensor1d([action.id], 'int32'),
                        this.config.actionSpace,
                        1,
                        0,
                        'float32'
                    )))
            for (let c = actionHistory.length; c < numUnrollSteps; c++) {
                tfActionHistory.push(tf.zeros([this.config.actionSpace]))
            }
            const target = gameHistory.makeTarget(position, numUnrollSteps, tdSteps, this.config.discount)
            return new Batch(gameHistory.makeImage(position), actionHistory, tfActionHistory, target)
        })
    }

    public loadSavedGames(
        environment: Environment<State>
    ): void {
        try {
            const json = fs.readFileSync('./data/games.json', {encoding: 'utf8'})
            if (json !== null) {
                this.buffer = new GameHistory(environment).deserialize(json)
                this.totalSamples = this.buffer.reduce((sum, game) => sum + game.rootValues.length, 0)
                this.numPlayedGames = this.buffer.length
                this.numPlayedSteps = this.totalSamples
            }
        } catch (e) {
            debug(e)
        }
    }

    public storeSavedGames(): void {
        const stream = JSON.stringify(this.buffer.map(gh => gh.serialize()))
        fs.writeFileSync('./data/games.json', stream, 'utf8')
    }

    /**
     * statistics - return the percentage of game wins for player 1
     */
    public statistics(): number {
        const player1WinTotal = this.buffer.reduce(
            (s, game) => {
                const winner = (game.toPlayHistory.at(-1) ?? 0) * (game.rewards.at(-1) ?? 0)
                return s + (winner > 0 ? 1 : 0)
            }, 0)
        return player1WinTotal / this.buffer.length * 100
    }

    /**
     * sampleGame - Sample game from buffer either uniformly or according to some priority.
     * See paper appendix Training.
     * @param forceUniform Flag to force a uniform selection randomly
     * @private
     */
    private sampleGame(forceUniform: boolean): MuZeroGameSample<State> {
        const prioritizedSample = this.config.prioritizedReplay && !forceUniform
        let gameProb
        let gameIndex = 0
        if (prioritizedSample) {
            const gameProbs = this.buffer.map(gameHistory => gameHistory.gamePriority)
            // First, we loop the replay buffer to count up the total game priorities
            const total = gameProbs.reduce((s, p) => s + p, 0)
            // Total in hand, we can now pick a random value akin to our
            // random index from before.
            let threshold = Math.random() * total
            // Now we just need to loop through the replay buffer one more time
            // until we discover which value would live within this
            // particular threshold. Stop before last data.old.copy(1) set since there will be no need
            // for checking the threshold if we get so far
            for (; gameIndex < gameProbs.length - 1; ++gameIndex) {
                // Add the weight to our running total.
                threshold -= gameProbs[gameIndex]
                // If this value falls within the threshold, we're done!
                if (threshold < 0) {
                    break
                }
            }
            gameProb = gameProbs[gameIndex] / total
        } else {
            gameProb = 1 / this.buffer.length
            gameIndex = Math.floor(Math.random() * this.buffer.length)
        }
        return new MuZeroGameSample(this.buffer[gameIndex], gameProb)
    }

    /**
     * samplePosition - Sample position from game either uniformly or according to some priority
     * @param gameHistory
     * @param forceUniform Flag to force a uniform selection randomly
     * @private
     */
    private samplePosition(gameHistory: GameHistory<State>, forceUniform: boolean): MuZeroPositionSample {
        const prioritizedSample = this.config.prioritizedReplay && !forceUniform
        let positionProb
        let positionIndex = 0
        if (prioritizedSample) {
            // First, we loop the game history datasets to count up the total weight of priorities (discounted target deviations)
            const total = gameHistory.priorities.reduce((s, p) => s + p, 0)
            // Total in hand, we can now pick a random value akin to our
            // random index from before.
            let threshold = Math.random() * total
            // Now we just need to loop through the main data.old.copy(1) one more time
            // until we discover which value would live within this
            // particular threshold. Stop before last data.old.copy(1) set since there will be no need
            // for checking the threshold if we get so far
            for (; positionIndex < gameHistory.priorities.length - 1; ++positionIndex) {
                // Reduce our running total with the priority
                threshold -= gameHistory.priorities[positionIndex]
                // If this value falls within the threshold, we're done!
                if (threshold < 0) {
                    break
                }
            }
            positionProb = gameHistory.priorities[positionIndex] / total
        } else {
            positionProb = 1 / gameHistory.rootValues.length
            positionIndex = Math.floor(Math.random() * gameHistory.rootValues.length)
        }
        return new MuZeroPositionSample(positionIndex, positionProb)
    }
}
