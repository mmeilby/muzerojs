import { GameHistory } from '../selfplay/gamehistory'
import { Batch } from './batch'
import { GameSample } from './gamesample'
import { MuZeroPositionSample } from './positionsample'
import fs from 'fs'
import debugFactory from 'debug'
import { Environment } from '../games/core/environment'
import { ObservationModel } from '../games/core/model'
import { Config } from '../games/core/config'
import { Actionwise } from '../games/core/actionwise'
import { Statewise } from '../games/core/statewise'
import { Observation } from '../networks/nnet'
const debug = debugFactory('muzero:replaybuffer:module')

/**
 * Replay Buffer
 */

export class ReplayBuffer<State extends Statewise, Action extends Actionwise> {
  private buffer: Array<GameHistory<State, Action>>

  public numPlayedGames: number
  public numPlayedSteps: number
  public totalSamples: number

  /**
   *
   * @param config
   * @param config.replayBufferSize Number of self-play games to keep in the replay buffer
   * @param config.actionSpace Number of all possible actions
   * @param config.tdSteps Number of steps in the future to take into account for calculating
   * the target value
   * @param config.batchSize Number of parts of games to train on at each training step
   * @param config.numUnrollSteps Number of game moves to keep for every batch element
   * @param config.stackedObservations Number of previous observations and previous actions
   * to add to the current observation
   * @param config.discount Chronological discount of the reward
   * @param config.prioritizedReplay Prioritized Replay (See paper appendix Training),
   * select in priority the elements in the replay buffer which are unexpected for the network
   * @param config.priorityAlpha How much prioritization is used, 0 corresponding to the uniform case,
   * paper suggests 1.0
   */
  public constructor (
    private readonly config: Config
  ) {
    this.buffer = []
    this.numPlayedGames = 0
    this.numPlayedSteps = 0
    this.totalSamples = 0
  }

  /**
   * saveGame
   * Save a game in the replay buffer (generated by selfPlay)
   * @param gameHistory
   */
  public saveGame (gameHistory: GameHistory<State, Action>): void {
    /*
    if (this.config.prioritizedReplay) {
      // Initial priorities for the prioritized replay (See paper appendix Training)
      // For each game position calculate the absolute deviation from target value. Largest deviation has priority
      const priorities: number[] = []
      gameHistory.rootValues.forEach((rootValue, i) => {
        const targetValue = gameHistory.computeTargetValue(i, this.config.tdSteps, this.config.discount)
        const priority = Math.pow(Math.abs(rootValue - targetValue), this.config.priorityAlpha)
        priorities.push(priority)
      })
      // Deviations are saved for later priority sorting
      gameHistory.priorities = priorities
      gameHistory.gamePriority = gameHistory.priorities.reduce((m, p) => Math.max(m, p), 0)
    }
    */
    if (this.buffer.length >= this.config.replayBufferSize) {
      const delGameHistory = this.buffer.shift()
      if (delGameHistory != null) {
        this.totalSamples -= delGameHistory.recordedSteps()
      }
    }
    this.buffer.push(gameHistory)
    this.numPlayedGames++
    this.numPlayedSteps += gameHistory.recordedSteps()
    this.totalSamples += gameHistory.recordedSteps()
    if (this.numPlayedGames % 25 === 0) {
      //      this.storeSavedGames()
    }
  }

  /**
   * sampleBatch
   * Get a sample batch from the replay buffer (used for training)
   * @param numUnrollSteps Number of game moves to keep for every batch element
   * @param tdSteps Number of steps in the future to take into account for calculating the target value
   * @param forceUniform Flag to force a uniform selection randomly
   * @return MuZeroBatch[] Sample batch - list of batch elements (batchSize length)
   */
  public sampleBatch (numUnrollSteps: number, tdSteps: number, forceUniform = false): Array<Batch<Action>> {
    const gameSamples: Array<GameSample<State, Action>> = []
    if (this.numPlayedGames > 0) {
      for (let c = 0; c < (this.config.batchSize); c++) {
        gameSamples.push(this.sampleGame(forceUniform))
      }
    }
    //    debug('Sample %d games', this.batchSize)
    return gameSamples.map(g => {
      const gameHistory = g.gameHistory
      //      debug(game.env.toString(game.state))
      //      debug(game.state.toString())
      const index = this.samplePosition(gameHistory, forceUniform).position
      const target = gameHistory.makeTarget(index, numUnrollSteps, tdSteps, this.config.discount)
      const validTargets = target.map((t, i) => t.policy.length > 0 ? i : -1).filter(v => v >= 0)
      const observations: Observation[] = validTargets.map(i => gameHistory.makeImage(index + i))
      return new Batch(observations, target.filter(t => t.policy.length > 0))
    })
  }

  /**
   * sampleGame - Sample game from buffer either uniformly or according to some priority.
   * See paper appendix Training.
   * @param forceUniform Flag to force a uniform selection randomly
   * @private
   */
  private sampleGame (forceUniform: boolean): GameSample<State, Action> {
    const prioritizedSample = this.config.prioritizedReplay && !forceUniform
    let gameProb
    let gameIndex = 0
    //    if (prioritizedSample) {
    /*
      const gameProbs = this.buffer.map(gameHistory => gameHistory.gamePriority)
      // First, we loop the replay buffer to count up the total game priorities
      const total = gameProbs.reduce((s, p) => s + p, 0)
      // Total in hand, we can now pick a random value akin to our
      // random index from before.
      let threshold = Math.random() * total
      // Now we just need to loop through the replay buffer one more time
      // until we discover which value would live within this
      // particular threshold. Stop before last data.copy(1) set since there will be no need
      // for checking the threshold if we get so far
      for (; gameIndex < gameProbs.length - 1; ++gameIndex) {
        // Add the weight to our running total.
        threshold -= gameProbs[gameIndex]
        // If this value falls within the threshold, we're done!
        if (threshold < 0) {
          break
        }
      }
      gameProb = gameProbs[gameIndex] / total
      */
    //    } else {
    gameProb = 1 / this.buffer.length
    gameIndex = Math.floor(Math.random() * this.buffer.length)
    //    }
    return new GameSample(this.buffer[gameIndex], gameProb)
  }

  /**
   * samplePosition - Sample position from game either uniformly or according to some priority
   * @param gameHistory
   * @param forceUniform Flag to force a uniform selection randomly
   * @private
   */
  private samplePosition (gameHistory: GameHistory<State, Action>, forceUniform: boolean): MuZeroPositionSample {
    const prioritizedSample = this.config.prioritizedReplay && !forceUniform
    let positionProb
    let positionIndex = 0
    /*
    if (prioritizedSample) {
      // First, we loop the game history datasets to count up the total weight of priorities (discounted target deviations)
      const total = gameHistory.priorities.reduce((s, p) => s + p, 0)
      // Total in hand, we can now pick a random value akin to our
      // random index from before.
      let threshold = Math.random() * total
      // Now we just need to loop through the main data.copy(1) one more time
      // until we discover which value would live within this
      // particular threshold. Stop before last data.copy(1) set since there will be no need
      // for checking the threshold if we get so far
      for (; positionIndex < gameHistory.priorities.length - 1; ++positionIndex) {
        // Reduce our running total with the priority
        threshold -= gameHistory.priorities[positionIndex]
        // If this value falls within the threshold, we're done!
        if (threshold < 0) {
          break
        }
      }
      positionProb = gameHistory.priorities[positionIndex] / total
    } else {
     */
    positionProb = 1 / gameHistory.recordedSteps()
    positionIndex = Math.floor(Math.random() * gameHistory.recordedSteps())
    //    }
    return new MuZeroPositionSample(positionIndex, positionProb)
  }

  public loadSavedGames (
    environment: Environment<State, Action>,
    model: ObservationModel<State>
  ): void {
    try {
      const json = fs.readFileSync('./data/games.json', { encoding: 'utf8' })
      if (json !== null) {
        this.buffer = new GameHistory(environment, model).deserialize(json)
        this.totalSamples = this.buffer.reduce((sum, game) => sum + game.recordedSteps(), 0)
        this.numPlayedGames = this.buffer.length
        this.numPlayedSteps = this.totalSamples
      }
    } catch (e) {
      debug(e)
    }
  }

  public storeSavedGames (): void {
    const stream = JSON.stringify(this.buffer.map(gh => gh.serialize()))
    fs.writeFileSync('./data/games.json', stream, 'utf8')
  }
}
